---
title: "Practical Machine Learning  - Prediction Assignment"
author: "James Portman" 
date: "March 4, 2016"
output: html_document
---

### Background
By using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. Six participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The data is from accelerometers on the belt, forearm, arm, and dumbell of the participants. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

### Goal
Predict the manner in which the participants did their exercises. This paper describes the prediction model built, how cross validation was used, and what the expected sample error was. The prediction model will be used to predict 20 different test cases. <br>
The training data used was copied locally from: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv> <br>
The test data used was copied locally from: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

### Contents
1) Preparation of Datasets<br>
2) Training a Prediction Model<br>
3) Model Evaluation<br>
4) Prediction on Validation data<br>
5) Prediction on Test data<br>
6) Conclusions<br>
7) Coursera Submission<br>

```{r, echo=FALSE}
library(caret)
```

### 1) Preparation of Datasets
#### 1.1 Load data and replace #DIV/0! with an NA value.
```{r}
trainURL <- "/Users/admin/Documents/training.csv"
testURL <- "/Users/admin/Documents/test.csv"
training <- read.csv(trainURL, na.strings=c("#DIV/0!"), row.names = 1)
testing <- read.csv(testURL, na.strings=c("#DIV/0!"), row.names = 1)

dim(training)
```
There are 19,622 observations across 159 factors.<br>

Distribution of the five measured stances A,B,C,D,E is:
```{r}

```

#### 1.2 Cleanse data

```{r}

# Remove variables that are irrelevant : user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and num_window 
training <- training[,-c(1:7)]
testing <- testing[,-c(1:7)]

# Exclude near zero variance features
nzvColumns <- nearZeroVar(training)
training <- training[, -nzvColumns]

# Exclude features with NA
training <- training[, which(as.numeric(colSums(is.na(training)))==0)]

dim(training)
```

There are 52 factors after cleansing the data.<br><br>

#### 1.3 Partition Training data into Training and Validation sets
```{r}
# Partition into training and validating sets
trainSet <- training[sample(nrow(training), 14000), ] # Pick 1400 random observations for our Training set.
validationSet <- training[sample(nrow(training), 5000), ] # Pick 5000 random observations for our Validation set.
```

### 2) Training a Prediction Model
```{r}
lda_model <- train(classe ~ ., data=trainSet, method="lda")
lda_Accuracy <- confusionMatrix(trainSet$classe,predict(lda_model,trainSet))$overall[1]
lda_Accuracy
```
 
```{r, echo=FALSE}       
gbm_model <- train(classe ~ ., data=trainSet, method="gbm")
```

```{r}
#gbm_model <- train(classe ~ ., data=trainSet, method="gbm")
gbm_Accuracy <- confusionMatrix(trainSet$classe,predict(gbm_model,trainSet))$overall[1]
gbm_Accuracy 

rf_model <- train(classe ~ ., data=trainSet, method="rf")
rf_Accuracy <- confusionMatrix(trainSet$classe,predict(rf_model,trainSet))$overall[1]
rf_Accuracy
# rf_Accuracy is 1.
```

### 3) Model Evaluation
Base on the Accuracy results above, the Random Forest model was more accurate that the Gradient boosting model or the Linear Discriminant Analysis model. <br>

Let's review the top 20 variables.
```{r}
rfObject <- varImp(rf_model)
plot(rfObject, main = "Top 20 Variables", top = 20)
```

### 4) Prediction on Validation data
Let's see what our out-of-sample accuracy is by seeing how our model performs on our validation set that we held out from our training set. <br>

```{r}
pValidation <- predict(rf_model, validationSet)
validation_rf_Accuracy <- confusionMatrix(pValidation, validationSet$classe)$overall[1]
validation_rf_Accuracy
```

### 5) Prediction on Test data 
The prediction of our algorithm for the test set is:<br>
```{r}
pTest <- predict(rf_model, testing)
pTest
```

### 6) Conclusions 
The Random Forest method worked with an accuracy on the Test set with an out of sample error of 99.8%.<br>

Randoms forests are good at modelling large number of factors when the relationships between factors is not known. In this project, a Random forest model was able to handle unscaled variables and categorical variables.<br>

### 7) Coursera Submission 
```{r}
# Save the output to files according to instructions and post to the Coursera Submission page.
answers <- as.vector(pTest)
pml_write_files = function(x) {
    n = length(x)
    for (i in 1:n) {
        filename = paste0("problem_id_", i, ".txt")
        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, 
            col.names = FALSE)
    }
}

pml_write_files(answers)
```
